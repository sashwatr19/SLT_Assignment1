{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data\n",
    "train_data_small = pd.read_csv(r\"MNIST_train_small.csv\")\n",
    "test_data_small = pd.read_csv(r\"MNIST_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_mode(object):\n",
    "    modes = object.mode()\n",
    "    # If modes is a Series, take the first value; if it's a single value, return it directly\n",
    "    mode_value = modes.iloc[0] if isinstance(modes, pd.Series) else modes\n",
    "\n",
    "    return mode_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Func to extract features and labels form dataset\n",
    "def get_features_targets(data):\n",
    "\n",
    "    targets= data.iloc[:, 0]\n",
    "    features= data.iloc[:, 1:]\n",
    "    features.columns = list(range(features.shape[1]))\n",
    "\n",
    "    return features, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create KNN Classifier\n",
    "\n",
    "class KNN:\n",
    "\n",
    "    # Initialize Object\n",
    "    def __init__(self, k, distance):\n",
    "        if distance == 'l2':\n",
    "            self.norm = 2\n",
    "        self.features = None\n",
    "        self.labels = None\n",
    "        self.k = k\n",
    "\n",
    "    # Fit to training set\n",
    "    def fit(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "        \n",
    "    def predict_one(self, x, mult=None):\n",
    "\n",
    "        distances = self.features.progress_apply(lambda row: np.linalg.norm(row.sub(x), ord=self.norm), axis=1)\n",
    "        distances = distances.sort_values().loc[distances!=0]\n",
    "\n",
    "        if mult:\n",
    "\n",
    "            # Output max neighbours in k list\n",
    "            k_neighbours = distances[:np.max(self.k)].index\n",
    "            k_neighbours_labels = self.labels.loc[k_neighbours]\n",
    "            predicted_labels = []\n",
    "            \n",
    "            for i in self.k:       \n",
    "                relevant_neighbours = pd.Series(list(k_neighbours_labels)[:i])      \n",
    "                predicted_k = custom_mode(relevant_neighbours)\n",
    "                predicted_labels.append(predicted_k)\n",
    "\n",
    "            return predicted_labels\n",
    "        \n",
    "        else:\n",
    "            k_neighbours = distances[:self.k].index\n",
    "            k_neighbours_labels = self.labels.loc[k_neighbours]\n",
    "            prediction_label = custom_mode(k_neighbours_labels)\n",
    "\n",
    "            return prediction_label\n",
    "    \n",
    "    # Prediction for dataset\n",
    "    def predict(self, x, mult=None):\n",
    "        predictions = x.progress_apply(lambda row: self.predict_one(row, mult), axis=1)\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    # Error score    \n",
    "    def error_score(self, predicted, data_label):\n",
    "        difference = predicted.sub(data_label)\n",
    "        error = len(difference.loc[difference!=0]) / len(data_label)\n",
    "\n",
    "        return error\n",
    "    \n",
    "    # Error scores for multiple ks\n",
    "    def error_mult(self, predicted, data_labels):\n",
    "\n",
    "        if isinstance(self.k, (np.ndarray, list)):\n",
    "            \n",
    "            differences = [pr.sub(data_labels) for pr in predicted]\n",
    "            errors = [len(d.loc[d!=0]) / len(d) for d in differences]\n",
    "                \n",
    "            return errors\n",
    "            \n",
    "        else:\n",
    "            print(\"This method is used for multiple K\")\n",
    "            pass\n",
    "        \n",
    "    def loocv_score(self):\n",
    "        k_list = self.k\n",
    "        big_data_x = self.features.copy()\n",
    "        big_data_y = self.labels.copy()\n",
    "        loocv_errors = np.zeros(len(k_list))\n",
    "        \n",
    "        for i in range(len(self.features)):\n",
    "            big_data_x_copy = big_data_x.copy()\n",
    "            big_data_y_copy = big_data_y.copy()\n",
    "            \n",
    "            current_data_x = big_data_x_copy.drop(index=i)\n",
    "            current_data_y = big_data_y_copy.drop(index=i)\n",
    "            \n",
    "            validation_x = big_data_x_copy.loc[i]\n",
    "            validation_y = big_data_y_copy.loc[i]\n",
    "            # print('The value of y to be verified is ', validation_y)\n",
    "            \n",
    "            self.fit(current_data_x, current_data_y)\n",
    "            \n",
    "            current_predictions = self.predict_one(validation_x, True)\n",
    "            \n",
    "            loocv_err = np.array([int(pr != validation_y) for pr in current_predictions])\n",
    "            \n",
    "            loocv_errors = loocv_errors + loocv_err\n",
    "            \n",
    "        return loocv_errors / len(self.features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get features and labels for datasets\n",
    "train_small_X, train_small_y = get_features_targets(train_data_small)\n",
    "test_small_X, test_small_y = get_features_targets(test_data_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create KNN instance to get error score for multiple k\n",
    "k_list = np.array([i for i in range(1,20)])\n",
    "KNN_mult = KNN(k=k_list, distance='l2')\n",
    "KNN_mult.fit(train_small_X, train_small_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate error scores\n",
    "errors_training = KNN_mult.error_mult(train_small_X, train_small_y)\n",
    "errors_testing = KNN_mult.error_mult(test_small_X, test_small_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot errors\n",
    "plt.plot(k_list, errors_training, color='m', label='Risk for Training Set')\n",
    "plt.plot(k_list, errors_testing, color='g', label='Risk for Testing Set')\n",
    "plt.title('Emperical Risk for a Range of Ks')\n",
    "plt.ylabel('Error Rate')\n",
    "plt.xlabel('K-Value')\n",
    "plt.xticks(list(range(1,21)))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get LOOCV risk scores\n",
    "loocv_small = KNN_mult.loocv_score()\n",
    "print('The errors for each k are given by ', loocv_small)\n",
    "\n",
    "# Plot errors with loocv\n",
    "plt.plot(k_list, loocv_small, color='m', label='Risk for Train Set with LOOCV')\n",
    "plt.plot(k_list, errors_testing, color='g', label='Risk for Test Set')\n",
    "plt.title('Empirical Risk for a Range of Ks')\n",
    "plt.ylabel('Empirical Risk')\n",
    "plt.xlabel('K-Value')\n",
    "plt.ylim((0,0.15))\n",
    "plt.xlim((1,20))\n",
    "plt.xticks(list(range(1,21)))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing implementation\n",
    "train_small_X_binary = (train_small_X > 128).astype(int)\n",
    "\n",
    "pca = PCA(n_components=50)\n",
    "train_small_X_pca = pca.fit_transform(train_small_X_binary)\n",
    "\n",
    "train_small_X_pca = pd.DataFrame(train_small_X_pca, index=train_small_X_binary.index)\n",
    "\n",
    "KNN_LOOCV_preprocessed = KNN(k_list, 'l2')\n",
    "KNN_LOOCV_preprocessed.fit(train_small_X_pca, train_small_y)\n",
    "\n",
    "loocv_preprocessed = KNN_LOOCV_preprocessed.loocv()\n",
    "\n",
    "print('LOOCV Error Rates after Preprocessing:', loocv_preprocessed)\n",
    "plt.plot(k_list, loocv_small, label='Before Preprocessing', linestyle='dashed', marker='o', color='red')\n",
    "plt.plot(k_list, loocv_preprocessed, label='After Preprocessing', linestyle='solid', marker='s', color='blue')\n",
    "\n",
    "plt.xlabel('k-Value')\n",
    "plt.ylabel('LOOCV Error Rate')\n",
    "plt.title('Before vs After Preprocessing')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implementation broke down while running on the large dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation Two and Three"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uses new KNN classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create KNN Classifier\n",
    "\n",
    "class KNN:\n",
    "\n",
    "    # Initialize Object\n",
    "    def __init__(self, k, distance=None):\n",
    "        if distance == None:\n",
    "            self.norm = 2\n",
    "        else:\n",
    "            self.norm = distance\n",
    "        self.features = None\n",
    "        self.labels = None\n",
    "        self.k = k\n",
    "\n",
    "    # Fit to training set\n",
    "    def fit(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    \n",
    "    # Prediction for dataset\n",
    "    def predict(self, x, mult=None, all=None):\n",
    "\n",
    "        if mult == None and all == None:\n",
    "            predictions = cdist(x, self.features, metric='minkowski', p=self.norm)\n",
    "            pred_ind = np.argsort(predictions, axis=1)[:, :self.k]\n",
    "            preds = pd.DataFrame(self.labels[pred_ind]).mode(axis=1).iloc[:, 0].values\n",
    "\n",
    "        elif all == 'all':\n",
    "            preds = cdist(x, self.features, metric='minkowski', p=self.norm)\n",
    "        \n",
    "        else:\n",
    "            predictions = cdist(x, self.features, metric='minkowski', p=self.norm)\n",
    "            pred_ind = np.argsort(predictions, axis=1)[:, :max(self.k)]\n",
    "            preds = self.labels[pred_ind]\n",
    "\n",
    "        return preds\n",
    "    \n",
    "    # Prediction for multiple K\n",
    "    def error_mult(self, x, true_labels, prediction_data = None):\n",
    "\n",
    "        if isinstance(self.k, (np.ndarray, list)):\n",
    "            preds = self.predict(x, mult=True)\n",
    "            pred_df = pd.DataFrame(columns=[str(i) for i in self.k])\n",
    "            errors_k = []\n",
    "\n",
    "            for i in self.k:\n",
    "                pred_labels = pd.DataFrame(preds).iloc[:,:i].mode(axis=1).iloc[:,0].values\n",
    "                pred_df[str(i)] = pred_labels\n",
    "                errors_k.append(self.error_score(pred_labels, true_labels))\n",
    "            \n",
    "            if prediction_data != None :\n",
    "                return pred_df, errors_k\n",
    "            \n",
    "            if prediction_data == None:\n",
    "                return errors_k\n",
    "        \n",
    "        else:\n",
    "\n",
    "            print(\"This method is used for multiple K. Enter a list of K-values.\")\n",
    "            pass\n",
    "\n",
    "    # Error score\n",
    "    def error_score(self, predicted, data_label):\n",
    "        errors = predicted - data_label\n",
    "        error_score = len(errors[errors!=0])/len(data_label)\n",
    "\n",
    "        return error_score\n",
    "\n",
    "    # Leave One Out CV Score\n",
    "    def loocv_score(self):\n",
    "\n",
    "        all_pred = self.predict(self.features, all='all')\n",
    "\n",
    "        if isinstance(self.k, (np.ndarray, list)):\n",
    "            k_errors = []\n",
    "            sorted_ind = np.argsort(all_pred, axis=1)\n",
    "\n",
    "            for i in self.k:\n",
    "                pred_ind = sorted_ind[:, 1:i+1]\n",
    "                preds = pd.DataFrame(self.labels[pred_ind]).mode(axis=1).iloc[:, 0].values\n",
    "                k_errors.append(self.error_score(preds, self.labels))\n",
    "\n",
    "            return k_errors\n",
    "        \n",
    "        else:\n",
    "            pred_ind = np.argsort(all_pred, axis=1)[:, 1:self.k+1]\n",
    "            preds = pd.DataFrame(self.labels[pred_ind]).mode(axis=1).iloc[:, 0].values\n",
    "\n",
    "            return self.error_score(preds, self.labels)\n",
    "        \n",
    "    # Leave One Out CV Score - 2\n",
    "    def loocv_score_big(self):\n",
    "\n",
    "        data_split = [self.features[(i*10000):((i+1)*10000)] for i in range(0,6)]\n",
    "        label_split = [self.labels[(i*10000):((i+1)*10000)] for i in range(0,6)]\n",
    "\n",
    "        k_errors = np.zeros(len(self.k))\n",
    "\n",
    "        if isinstance(self.k, (np.ndarray, list)):\n",
    "\n",
    "            for i in tqdm(range(len(data_split))):\n",
    "                curr_pred = self.predict(data_split[i], all='all')\n",
    "\n",
    "                k_curr_errors = np.zeros(len(self.k))\n",
    "\n",
    "                sorted_ind = np.argsort(curr_pred, axis=1)\n",
    "\n",
    "                for j, l in enumerate(self.k):\n",
    "                    pred_ind = sorted_ind[:, 1:l+1]\n",
    "                    preds = pd.DataFrame(self.labels[pred_ind]).mode(axis=1).iloc[:, 0].values\n",
    "                    k_curr_errors[j] = self.error_score(preds, label_split[i])\n",
    "\n",
    "                k_errors += k_curr_errors\n",
    "\n",
    "            return k_errors/len(data_split)\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            for i in tqdm(range(len(data_split))):\n",
    "                curr_pred = self.predict(data_split[i], all='all')\n",
    "\n",
    "                k_curr_errors = np.zeros(len(self.k))\n",
    "\n",
    "                sorted_ind = np.argsort(curr_pred, axis=1)\n",
    "                pred_ind = sorted_ind[:, 1:self.k+1]\n",
    "                preds = pd.DataFrame(self.labels[pred_ind]).mode(axis=1).iloc[:, 0].values\n",
    "                k_curr_errors = self.error_score(preds, label_split[i])\n",
    "\n",
    "                k_errors += k_curr_errors\n",
    "\n",
    "            return k_errors/len(data_split)\n",
    "        \n",
    "    # Search for best metric \n",
    "    def grid_search_dist(self, dist_range):\n",
    "\n",
    "        dist_df = pd.DataFrame(columns=[str(i) for i in dist_range])\n",
    "\n",
    "        for i in tqdm(dist_range): \n",
    "            self.norm = i\n",
    "            dist_df[str(i)] = self.loocv_score()\n",
    "\n",
    "        return dist_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do everything as before\n",
    "\n",
    "k_list = np.array([i for i in range(1,21)])\n",
    "KNN_mult = KNN(k=k_list)\n",
    "KNN_mult.fit(train_small_X, train_small_y)\n",
    "\n",
    "errors_training = KNN_mult.error_mult(train_small_X, train_small_y)\n",
    "errors_testing = KNN_mult.error_mult(test_small_X, test_small_y)\n",
    "\n",
    "plt.plot(k_list, errors_training, color='m', label='Risk for Train Set')\n",
    "plt.plot(k_list, errors_testing, color='g', label='Risk for Test Set')\n",
    "plt.title('Empirical Risk for a Range of Ks')\n",
    "plt.ylabel('Empirical Risk')\n",
    "plt.xlabel('K-Value')\n",
    "plt.ylim((0,0.15))\n",
    "plt.xlim((1,20))\n",
    "plt.xticks(k_list)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "LOOCV_score = KNN_mult.loocv_score()\n",
    "\n",
    "plt.plot(k_list, LOOCV_score, color='m', label='Risk for Train Set with LOOCV')\n",
    "plt.plot(k_list, errors_testing, color='g', label='Risk for Test Set')\n",
    "plt.title('Empirical Risk for a Range of Ks')\n",
    "plt.ylabel('Empirical Risk')\n",
    "plt.xlabel('K-Value')\n",
    "plt.ylim((0,0.15))\n",
    "plt.xlim((1,20))\n",
    "plt.xticks(list(range(1,21)))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search for best k and p\n",
    "dist_range = [i for i in range(1,16)]\n",
    "dist_df = KNN_mult.grid_search_dist(dist_range=dist_range)\n",
    "\n",
    "# Plot heatmap\n",
    "plt.imshow(dist_df, cmap='magma')\n",
    "plt.xlabel('P-Value')\n",
    "plt.ylabel('K-Value')\n",
    "plt.xticks(ticks=range(0,15), labels=dist_range)\n",
    "plt.yticks(ticks=range(0,20), labels=k_list)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "# Get position of minimum risk\n",
    "min_error = dist_df.to_numpy().min()\n",
    "print(f\"{min_error:.4f} error rate acheived at k={np.where(dist_df == min_error)[0][0]+1} and p={np.where(dist_df == min_error)[1][0]+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using large dataset\n",
    "train_data = pd.read_csv(r\"MNIST_train.csv\")\n",
    "test_data = pd.read_csv(r\"MNIST_test.csv\")\n",
    "\n",
    "# Get features and labels for datasets\n",
    "train_X, train_y = get_features_targets(train_data)\n",
    "test_X, test_y = get_features_targets(test_data)\n",
    "\n",
    "# Binarize data\n",
    "train_X_binary = (train_X > 128).astype(int)\n",
    "\n",
    "k_list = np.array([i for i in range(1,21)])\n",
    "\n",
    "# Create KNN instance\n",
    "KNN_big_loocv = KNN(k=k_list, distance=2)\n",
    "KNN_big_loocv.fit(train_X_binary, train_y)\n",
    "\n",
    "# Get loocv score\n",
    "KNN_big_loocv_scores = KNN_big_loocv.loocv_score()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code broke down at the last step which leads us to implementation three"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Three"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do as before\n",
    "# Using large dataset\n",
    "train_data = pd.read_csv(r\"MNIST_train.csv\")\n",
    "test_data = pd.read_csv(r\"MNIST_test.csv\")\n",
    "\n",
    "# Get features and labels for datasets\n",
    "train_X, train_y = get_features_targets(train_data)\n",
    "test_X, test_y = get_features_targets(test_data)\n",
    "\n",
    "# Binarize data\n",
    "train_X_binary = (train_X > 128).astype(int)\n",
    "\n",
    "k_list = np.array([i for i in range(1,21)])\n",
    "\n",
    "# Create KNN instance\n",
    "KNN_big_loocv = KNN(k=k_list, distance=2)\n",
    "KNN_big_loocv.fit(train_X_binary, train_y)\n",
    "\n",
    "# Get loocv score with new func\n",
    "KNN_big_loocv_scores = KNN_big_loocv.loocv_score_big()\n",
    "\n",
    "# Plot\n",
    "plt.plot(k_list, KNN_big_loocv_scores)\n",
    "plt.xticks(k_list)\n",
    "plt.ylim((0.03,0.06))\n",
    "plt.xlabel('K Value')\n",
    "plt.ylabel('Empirical Risk')\n",
    "plt.title('Empirical Risk with LOOCV')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict for test data\n",
    "test_X_binary = (test_X > 128).astype(int)\n",
    "\n",
    "KNN_test_big = KNN(k=3, distance=2)\n",
    "KNN_test_big.fit(train_X_binary, train_y)\n",
    "predictions_test = KNN_test_big.predict(test_X_binary) \n",
    "\n",
    "# Get error score\n",
    "error_test = KNN_test_big.error_score(predictions_test, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get error score for all k \n",
    "k_list = np.array([i for i in range(1,21)])\n",
    "KNN_test_mult = KNN(k=k_list, distance=2)\n",
    "KNN_test_mult.fit(train_X_binary, train_y)\n",
    "\n",
    "test_error_mult = KNN_test_mult.error_mult(test_X_binary, test_y)\n",
    "\n",
    "# Plot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(k_list, test_error_mult)\n",
    "plt.xticks(k_list)\n",
    "plt.ylim((0.04,0.055))\n",
    "plt.xlabel('K Value')\n",
    "plt.ylabel('Empirical Risk')\n",
    "plt.title('Empirical Risk on Entire Test Set')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SLT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
